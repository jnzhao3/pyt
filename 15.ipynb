{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902b87aa",
   "metadata": {},
   "source": [
    "# 15 MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a95cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    dropout_p: float = 0.0\n",
    "    bias: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f388a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(T: int, S: Optional[int] = None, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a bool mask [T, S] where True means ALLOW, False means BLOCK.\n",
    "    Causal means each query position t can attend to keys <= t (when S==T).\n",
    "    If S!=T, interpret keys as ordered and block future keys beyond query index where possible.\n",
    "    \"\"\"\n",
    "    if S is None:\n",
    "        S = T\n",
    "    m = torch.ones((T, S), dtype=torch.bool, device=device)\n",
    "    # Allow only j <= i for the overlapping region.\n",
    "    idx_i = torch.arange(T, device=device)[:, None]\n",
    "    idx_j = torch.arange(S, device=device)[None, :]\n",
    "    return m & (idx_j <= idx_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd49dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Interview scaffold: implement multi-head attention (self-attn and cross-attn).\n",
    "\n",
    "    Conventions:\n",
    "      - x: [B, T, E]\n",
    "      - kv: [B, S, E] (only for cross-attn; S may differ from T)\n",
    "      - attn_mask: [T, S] or [B, 1, T, S] (bool or float)\n",
    "      - key_padding_mask: [B, S] bool (True = keep, False = pad)  (or invert if you prefer; document!)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: AttentionConfig):\n",
    "        super().__init__()\n",
    "        assert cfg.embed_dim % cfg.num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.cfg = cfg\n",
    "        self.head_dim = cfg.embed_dim // cfg.num_heads\n",
    "\n",
    "        # Candidate should use these\n",
    "        self.q_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim, bias=cfg.bias)\n",
    "        self.k_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim, bias=cfg.bias)\n",
    "        self.v_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim, bias=cfg.bias)\n",
    "        self.out_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim, bias=cfg.bias)\n",
    "        self.dropout = nn.Dropout(cfg.dropout_p)\n",
    "\n",
    "    # ---------- helpers (candidate implements) ----------\n",
    "\n",
    "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: [B, T, E]\n",
    "        Returns:\n",
    "          xh: [B, H, T, Dh]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _merge_heads(self, xh: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          xh: [B, H, T, Dh]\n",
    "        Returns:\n",
    "          x:  [B, T, E]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _apply_masks(\n",
    "        self,\n",
    "        attn_logits: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor],\n",
    "        key_padding_mask: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply masks to attention logits before softmax.\n",
    "\n",
    "        Args:\n",
    "          attn_logits: [B, H, T, S]\n",
    "          attn_mask: optional, either\n",
    "            - bool mask where True means ALLOW, False means BLOCK\n",
    "              shape [T, S] or [B, 1, T, S]\n",
    "            - OR float additive mask (e.g., 0 for allow, -inf for block)\n",
    "              shape [T, S] or [B, 1, T, S]\n",
    "          key_padding_mask: optional bool mask [B, S] where True means token is real (keep),\n",
    "                            False means padding (block).\n",
    "                            (If you prefer the opposite convention, document and adjust tests.)\n",
    "        Returns:\n",
    "          masked_logits: [B, H, T, S]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # ---------- main API (candidate implements) ----------\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        kv: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Multi-head attention.\n",
    "\n",
    "        Self-attn when kv is None:\n",
    "          q,k,v all come from x.\n",
    "\n",
    "        Cross-attn when kv is not None:\n",
    "          q from x, k,v from kv.\n",
    "\n",
    "        Args:\n",
    "          x:  [B, T, E]\n",
    "          kv: [B, S, E] or None\n",
    "          attn_mask: optional [T, S] or [B, 1, T, S], bool or float additive\n",
    "          key_padding_mask: optional [B, S] bool (True keep, False pad)\n",
    "          need_weights: if True, also return attention probs averaged over heads [B, T, S]\n",
    "\n",
    "        Returns:\n",
    "          y: [B, T, E]\n",
    "          weights (optional): [B, T, S] (head-averaged attention probabilities)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f592d",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seed_all(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _assert_close(a: torch.Tensor, b: torch.Tensor, atol=1e-5, rtol=1e-5, msg=\"\"):\n",
    "    if not torch.allclose(a, b, atol=atol, rtol=rtol):\n",
    "        raise AssertionError(msg + f\"\\nmax abs diff: {(a-b).abs().max().item()}\")\n",
    "\n",
    "def run_mha_basic_tests():\n",
    "    \"\"\"\n",
    "    Keep these private or reveal progressively.\n",
    "    \"\"\"\n",
    "    _seed_all(0)\n",
    "    device = \"cpu\"\n",
    "\n",
    "    cfg = AttentionConfig(embed_dim=8, num_heads=2, dropout_p=0.0, bias=True)\n",
    "    mha = MultiHeadAttention(cfg).to(device)\n",
    "    mha.eval()\n",
    "\n",
    "    B, T, S, E = 2, 4, 5, cfg.embed_dim\n",
    "    x = torch.randn(B, T, E, device=device)\n",
    "    kv = torch.randn(B, S, E, device=device)\n",
    "\n",
    "    # ---- Test 1: output shape (self-attn) ----\n",
    "    y, w = mha(x, need_weights=True)\n",
    "    assert y.shape == (B, T, E), f\"self-attn output shape wrong: {y.shape}\"\n",
    "    assert w is not None and w.shape == (B, T, T), f\"weights shape wrong: {None if w is None else w.shape}\"\n",
    "    _assert_close(w.sum(dim=-1), torch.ones(B, T), msg=\"weights should sum to 1 over keys (self-attn)\")\n",
    "\n",
    "    # ---- Test 2: output shape (cross-attn) ----\n",
    "    y2, w2 = mha(x, kv=kv, need_weights=True)\n",
    "    assert y2.shape == (B, T, E), f\"cross-attn output shape wrong: {y2.shape}\"\n",
    "    assert w2 is not None and w2.shape == (B, T, S), f\"cross weights shape wrong: {None if w2 is None else w2.shape}\"\n",
    "    _assert_close(w2.sum(dim=-1), torch.ones(B, T), msg=\"weights should sum to 1 over keys (cross-attn)\")\n",
    "\n",
    "    # ---- Test 3: key padding mask blocks padded keys ----\n",
    "    # Mark last 2 keys as padding (False = pad)\n",
    "    kpm = torch.ones(B, S, dtype=torch.bool, device=device)\n",
    "    kpm[:, -2:] = False\n",
    "    y3, w3 = mha(x, kv=kv, key_padding_mask=kpm, need_weights=True)\n",
    "    assert (w3[..., -2:] < 1e-6).all(), \"padded keys should get ~0 attention probability\"\n",
    "\n",
    "    # ---- Test 4: causal mask blocks future keys (self-attn) ----\n",
    "    cm = causal_mask(T, device=device)  # [T, T], True allow\n",
    "    y4, w4 = mha(x, attn_mask=cm, need_weights=True)\n",
    "    # For each query t, positions > t should be ~0\n",
    "    triu = torch.triu(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=1)\n",
    "    assert (w4[:, triu] < 1e-6).all(), \"causal mask should block future positions\"\n",
    "\n",
    "    # ---- Test 5: determinism / stability sanity ----\n",
    "    # If x is all zeros and projections have bias, outputs should be consistent; mostly checks no NaNs.\n",
    "    x0 = torch.zeros(B, T, E, device=device)\n",
    "    y0, w0 = mha(x0, need_weights=True)\n",
    "    assert torch.isfinite(y0).all() and torch.isfinite(w0).all(), \"should not produce NaNs/Infs\"\n",
    "\n",
    "    print(\"All MHA basic tests passed âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mha_basic_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
